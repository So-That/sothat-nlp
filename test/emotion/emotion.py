# ===== ê°ì •ë¶„ì„ë§Œ ìˆ˜í–‰ (Hugging Face ê¸°ë°˜, JSON â†’ Elasticsearch ì €ì¥) =====
import json
import logging
from datetime import datetime
from transformers import pipeline
from elasticsearch import Elasticsearch

# Logging ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HuggingFaceSentimentAnalyzer:
    def __init__(self):
        # Hugging Face pipeline ë¡œë“œ
        self.sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

    def analyze_sentiment(self, text):
        try:
            results = self.sentiment_pipeline(text)
            result = results[0]
            label = result['label']
            score = result['score']

            if '1' in label or '2' in label:
                sentiment = "negative"
            elif '3' in label:
                sentiment = "neutral"
            else:
                sentiment = "positive"
        except Exception as e:
            logger.error(f"Hugging Face ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            sentiment = "neutral"
            score = 0.0

        return {
            "sentiment": sentiment,
            "confidence": round(score, 3)
        }

def analyze_from_json(json_path, es_index="youtube_sentiment", es_host="http://localhost:9200"):
    # Elasticsearch ì—°ê²°
    es = Elasticsearch(es_host)
    if not es.ping():
        logger.error("âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨")
        return

    analyzer = HuggingFaceSentimentAnalyzer()

    with open(json_path, "r", encoding="utf-8") as f:
        comments = json.load(f)

    for comment in comments:
        if "reply" not in comment:
            logger.warning("âš ï¸ 'reply' í•„ë“œ ì—†ìŒ: %s", comment)
            continue

        reply_text = comment["reply"]
        logger.info("ğŸ’¬ ê°ì • ë¶„ì„ ì¤‘: %s", reply_text)

        sentiment_result = analyzer.analyze_sentiment(reply_text)

        doc = {
            "original": comment,
            "sentiment": sentiment_result,
            "analyzed_at": datetime.utcnow().isoformat()
        }

        try:
            es.index(index=es_index, document=doc)
            logger.info("âœ… Elasticsearch ì €ì¥ ì™„ë£Œ: %s", doc)
        except Exception as e:
            logger.error("âŒ Elasticsearch ì €ì¥ ì‹¤íŒ¨: %s", e)

if __name__ == "__main__":
    
    json_file_path = "re_clean.json"  
    # analyze_from_json(json_file_path)
